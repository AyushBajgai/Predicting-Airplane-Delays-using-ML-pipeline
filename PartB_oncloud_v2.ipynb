{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Predicting Airplane Delays\n",
    "\n",
    "The goals of this notebook are:\n",
    "- Process and create a dataset from downloaded ZIP files\n",
    "- Exploratory data analysis (EDA)\n",
    "- Establish a baseline model and improve it\n",
    "\n",
    "## Introduction to business scenario\n",
    "You work for a travel booking website that is working to improve the customer experience for flights that were delayed. The company wants to create a feature to let customers know if the flight will be delayed due to weather when the customers are booking the flight to or from the busiest airports for domestic travel in the US. \n",
    "\n",
    "You are tasked with solving part of this problem by leveraging machine learning to identify whether the flight will be delayed due to weather. You have been given access to the a dataset of on-time performance of domestic flights operated by large air carriers. You can use this data to train a machine learning model to predict if the flight is going to be delayed for the busiest airports.\n",
    "\n",
    "### Dataset\n",
    "The provided dataset contains scheduled and actual departure and arrival times reported by certified US air carriers that account for at least 1 percent of domestic scheduled passenger revenues. The data was collected by the Office of Airline Information, Bureau of Transportation Statistics (BTS). The dataset contains date, time, origin, destination, airline, distance, and delay status of flights for flights between 2014 and 2018.\n",
    "The data are in 60 compressed files, where each file contains a CSV for the flight details in a month for the five years (from 2014 - 2018). The data can be downloaded from this [link](https://ucstaff-my.sharepoint.com/:f:/g/personal/ibrahim_radwan_canberra_edu_au/EhWeqeQsh-9Mr1fneZc9_0sBOBzEdXngvxFJtAlIa-eAgA?e=8ukWwa). Please download the data files and place them on a relative path. Dataset(s) used in this assignment were compiled by the Office of Airline Information, Bureau of Transportation Statistics (BTS), Airline On-Time Performance Data, available with the following [link](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the environment \n",
    "\n",
    "Use one of the labs which we have practised on with the Amazon Sagemakers where you perform the following steps:\n",
    "1. Start a lab.\n",
    "2. Create a notebook instance and name it \"oncloudproject\".\n",
    "3. Increase the used memory to 25 GB from the additional configurations.\n",
    "4. Open Jupyter Lab and upload this notebook into it.\n",
    "5. Upload the two combined CVS files (combined_csv_v1.csv and combined_csv_v2.csv), which you created in Part A of this project.\n",
    "\n",
    "**Note:** In case of the data is too much to be uploaded to the AWS, please use 20% of the data only for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Build and evaluate simple models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use linear learner estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey and to comments on the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os, io, json, time, tarfile, tempfile, shutil, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import re\n",
    "import pathlib\n",
    "from botocore.exceptions import ClientError\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker import image_uris, Session\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.amazon.linear_learner import LinearLearner\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "from sagemaker.amazon.linear_learner import LinearLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"combined_csv_20.csv\"\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target           0\n",
      "DayofMonth_24    0\n",
      "DayOfWeek_3      0\n",
      "DayOfWeek_2      0\n",
      "DayofMonth_31    0\n",
      "                ..\n",
      "Month_5          0\n",
      "Month_4          0\n",
      "Month_3          0\n",
      "Month_2          0\n",
      "is_holiday_1     0\n",
      "Length: 94, dtype: int64\n",
      "\n",
      "Columns with missing values only:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "After dropping NA missing values in dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Show number of NaNs in each column\n",
    "nan_counts = df.isna().sum().sort_values(ascending=False)\n",
    "print(nan_counts)\n",
    "\n",
    "# show columns that actually have NaNs\n",
    "nan_counts = nan_counts[nan_counts > 0]\n",
    "print(\"\\nColumns with missing values only:\")\n",
    "print(nan_counts)\n",
    "total_missing = df.isna().sum().sum()\n",
    "\n",
    "#Dropping NA\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# total missing\n",
    "total_missing = df.isna().sum().sum()\n",
    "print(\"\\nAfter dropping NA missing values in dataset:\", total_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numeric columns: ['target', 'Distance', 'DepHourofDay', 'AWND_O', 'AWND_O.1', 'PRCP_O', 'PRCP_O.1', 'TAVG_O', 'TAVG_O.1', 'AWND_D', 'AWND_D.1', 'PRCP_D', 'PRCP_D.1', 'TAVG_D', 'TAVG_D.1', 'SNOW_O', 'SNOW_O.1', 'SNOW_D', 'SNOW_D.1']\n",
      "Non-numeric columns: ['Year_2015', 'Year_2016', 'Year_2017', 'Year_2018', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO', 'is_holiday_1']\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=\"number\").columns\n",
    "\n",
    "print(\"\\nNumeric columns:\", list(numeric_cols))\n",
    "print(\"Non-numeric columns:\", list(non_numeric_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing target column\n",
    "# choose target column\n",
    "target_col = \"target\" if \"target\" in df.columns else (\"is_delay\" if \"is_delay\" in df.columns else None)\n",
    "\n",
    "if target_col is not None and 2 <= df[target_col].nunique() <= 20:\n",
    "    strat = df[target_col]\n",
    "else:\n",
    "    strat = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting 70-15-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/30 split\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.30,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=strat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 30%\n",
    "if target_col is not None and 2 <= temp_df[target_col].nunique() <= 20:\n",
    "    strat_temp = temp_df[target_col]\n",
    "else:\n",
    "    strat_temp = None\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=strat_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to train easier\n",
    "os.makedirs(\"splits\", exist_ok=True)\n",
    "train_df.to_csv(\"splits/train.csv\", index=False)\n",
    "val_df.to_csv(\"splits/val.csv\", index=False)\n",
    "test_df.to_csv(\"splits/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved splits to ./splits/\n",
      "train 70%: 146996 rows, 94 cols\n",
      " class distribution -> 0: 109906 (74.77%), 1: 37090 (25.23%)\n",
      "\n",
      "val 15%: 31499 rows, 94 cols\n",
      " class distribution -> 0: 23551 (74.77%), 1: 7948 (25.23%)\n",
      "\n",
      "test 15%: 31500 rows, 94 cols\n",
      " class distribution -> 0: 23552 (74.77%), 1: 7948 (25.23%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# report\n",
    "def describe(name, d, tgt):\n",
    "    print(f\"{name}: {len(d)} rows, {d.shape[1]} cols\")\n",
    "    if tgt and tgt in d.columns:\n",
    "        vc = d[tgt].value_counts(dropna=False).sort_index()\n",
    "        pct = (vc/len(d)*100).round(2)\n",
    "        print(\" class distribution ->\", \", \".join([f\"{k}: {vc[k]} ({pct[k]}%)\" for k in vc.index]))\n",
    "    print()\n",
    "\n",
    "target_col = \"target\"\n",
    "\n",
    "print(\"saved splits to ./splits/\")\n",
    "describe(\"train 70%\", train_df, target_col)\n",
    "describe(\"val 15%\", val_df,   target_col)\n",
    "describe(\"test 15%\", test_df,  target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"splits\", exist_ok=True)\n",
    "\n",
    "def put_label_first(df, label=\"target\"):\n",
    "    if label in df.columns:\n",
    "        cols = [label] + [c for c in df.columns if c != label]\n",
    "        return df[cols]\n",
    "    return df\n",
    "\n",
    "train_df = put_label_first(train_df, target_col)\n",
    "val_df   = put_label_first(val_df,   target_col)\n",
    "test_df  = put_label_first(test_df,  target_col)\n",
    "\n",
    "train_df.to_csv(\"splits/train.csv\", index=False, header=True) \n",
    "val_df.to_csv(  \"splits/val.csv\",   index=False, header=True)\n",
    "test_df.to_csv( \"splits/test.csv\",  index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Simple Storage Service in AWS cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    \"train\": \"splits/train.csv\",\n",
    "    \"val\":   \"splits/val.csv\",\n",
    "    \"test\":  \"splits/test.csv\",\n",
    "}\n",
    "\n",
    "bucket='c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft'\n",
    "prefix = \"linear-learner-delay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded → s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delaytrain.csv\n",
      "uploaded → s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delayval.csv\n",
      "uploaded → s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delaytest.csv\n",
      "\n",
      "S3 URIs for dataset:\n",
      "s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delaytrain.csv\n",
      "s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delayval.csv\n",
      "s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delaytest.csv\n"
     ]
    }
   ],
   "source": [
    "# Creating an S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def upload_and_verify(local_path, bucket, key):\n",
    "    s3.upload_file(local_path, bucket, key)\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "        print(f\"uploaded → s3://{bucket}/{key}\")\n",
    "    except ClientError as e:\n",
    "        print(f\"could not verify {key}: {e}\")\n",
    "\n",
    "# Upload each split\n",
    "for _, local_path in paths.items():\n",
    "    s3_key = f\"{prefix}{pathlib.Path(local_path).name}\"\n",
    "    upload_and_verify(local_path, bucket, s3_key)\n",
    "\n",
    "print(\"\\nS3 URIs for dataset:\")\n",
    "print(f\"s3://{bucket}/{prefix}train.csv\")\n",
    "print(f\"s3://{bucket}/{prefix}val.csv\")\n",
    "print(f\"s3://{bucket}/{prefix}test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "role = sagemaker.get_execution_role() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data channels\n",
    "train_s3 = f\"s3://{bucket}/{prefix}train.csv\"\n",
    "val_s3   = f\"s3://{bucket}/{prefix}val.csv\"\n",
    "test_s3  = f\"s3://{bucket}/{prefix}test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: us-east-1\n",
      "role: arn:aws:iam::416916046524:role/c182567a4701745l12017053t1w4-SageMakerExecutionRole-ndTKtIN19IYB\n",
      "s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delaytrain.csv\n",
      "s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delayval.csv\n",
      "s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delaytest.csv\n"
     ]
    }
   ],
   "source": [
    "train_input = TrainingInput(train_s3, content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(val_s3,   content_type=\"text/csv\")\n",
    "\n",
    "print(\"region:\", region)\n",
    "print(\"role:\", role)\n",
    "print(train_s3, val_s3, test_s3, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here Santize S3 CSV's due to previous error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def load_csv_with_header(key):\n",
    "    body = s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read()\n",
    "    return pd.read_csv(io.BytesIO(body))\n",
    "\n",
    "#Dropping the duplicate columns ending as .1\n",
    "def drop_dot1(df): return df[[c for c in df.columns if not c.endswith(\".1\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing only 0/1 numeric-only\n",
    "def bool_to_int(df):\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == bool: df[c] = df[c].astype(int)\n",
    "        elif df[c].dtype == object:\n",
    "            v = set(df[c].dropna().astype(str).str.lower().unique())\n",
    "            if v <= {\"true\",\"false\"}: df[c] = df[c].astype(str).str.lower().map({\"true\":1,\"false\":0})\n",
    "    return df\n",
    "\n",
    "def to_numeric(df): return df.apply(lambda col: pd.to_numeric(col, errors=\"coerce\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying label here\n",
    "def put_label_first(df, label=\"target\"):\n",
    "    if label in df.columns:\n",
    "        cols = [label] + [c for c in df.columns if c != label]\n",
    "        return df[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (146996, 86), pos_rate=0.252 → s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delaytrain.csv\n",
      "val: (31499, 86), pos_rate=0.252 → s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delayval.csv\n",
      "test: (31500, 86), pos_rate=0.252 → s3://c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft/linear-learner-delaytest.csv\n"
     ]
    }
   ],
   "source": [
    "#Sanitzing and uploacdig back without header\n",
    "def sanitize_one(name):\n",
    "    key = f\"{prefix}{name}.csv\"\n",
    "    df = load_csv_with_header(key)\n",
    "    df = drop_dot1(df)\n",
    "    df = bool_to_int(df)\n",
    "    df = to_numeric(df).fillna(0.0)\n",
    "    df = put_label_first(df, \"target\")\n",
    "\n",
    "    # force label to 0/1\n",
    "    y = df.iloc[:,0].values\n",
    "    uy = np.unique(y[~pd.isna(y)])\n",
    "    if not set(uy) <= {0,1}:\n",
    "        if len(uy)==2:\n",
    "            lo, hi = sorted(uy); df.iloc[:,0] = (df.iloc[:,0]==hi).astype(int)\n",
    "        else:\n",
    "            thr = np.nanmedian(y); df.iloc[:,0] = (df.iloc[:,0]>thr).astype(int)\n",
    "\n",
    "    # upload back WITHOUT header\n",
    "    local = f\"/tmp/{name}.csv\"\n",
    "    df.to_csv(local, index=False, header=False)\n",
    "    s3.upload_file(local, bucket, key)\n",
    "    print(f\"{name}: {df.shape}, pos_rate={df.iloc[:,0].mean():.3f} → s3://{bucket}/{key}\")\n",
    "\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    sanitize_one(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Sage Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = image_uris.retrieve(framework=\"linear-learner\", region=region)\n",
    "\n",
    "est = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}output/\",\n",
    "    sagemaker_session=sess,\n",
    "    max_run=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.set_hyperparameters(\n",
    "    predictor_type=\"binary_classifier\",\n",
    "    epochs=10,\n",
    "    mini_batch_size=256,\n",
    "    num_models=32,\n",
    "    loss=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: linear-learner-2025-10-27-09-30-36-524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-10-27 09:30:36 Starting - Starting the training job...........\n",
      "2025-10-27 09:31:37 Starting - Preparing the instances for training....\n",
      "2025-10-27 09:32:03 Downloading - Downloading input data........\n",
      "2025-10-27 09:32:48 Downloading - Downloading the training image...............\n",
      "2025-10-27 09:34:09 Training - Training image download completed. Training in progress..................................................\n",
      "2025-10-27 09:38:26 Uploading - Uploading generated training model...\n",
      "2025-10-27 09:38:43 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "train_input = TrainingInput(f\"s3://{bucket}/{prefix}train.csv\", content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(f\"s3://{bucket}/{prefix}val.csv\",   content_type=\"text/csv\")\n",
    "est.fit({\"train\": train_input, \"validation\": val_input}, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: linear-learner-2025-10-27-09-39-26-418\n",
      "INFO:sagemaker:Creating endpoint-config with name linear-learner-2025-10-27-09-39-26-418\n",
      "INFO:sagemaker:Creating endpoint with name linear-learner-2025-10-27-09-39-26-418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!endpoint: linear-learner-2025-10-27-09-39-26-418\n"
     ]
    }
   ],
   "source": [
    "predictor = est.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n",
    "endpoint_name = predictor.endpoint_name\n",
    "print(\"endpoint:\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: linear-learner-2025-10-27-09-43-29-405\n",
      "INFO:sagemaker:Creating transform job with name: linear-learner-2025-10-27-09-43-29-997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................\n",
      "..."
     ]
    }
   ],
   "source": [
    "obj = s3.get_object(Bucket=bucket, Key=f\"{prefix}test.csv\")\n",
    "test_df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()), header=None)\n",
    "y_true = test_df.iloc[:,0].astype(int).values\n",
    "X      = test_df.iloc[:,1:]\n",
    "\n",
    "local_x = \"/tmp/test_x.csv\"\n",
    "X.to_csv(local_x, header=False, index=False)\n",
    "s3.upload_file(local_x, bucket, f\"{prefix}test_x.csv\")\n",
    "test_x_s3 = f\"s3://{bucket}/{prefix}test_x.csv\"\n",
    "\n",
    "from sagemaker.transformer import Transformer\n",
    "transformer = est.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}batch_out/\",\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"application/jsonlines\",\n",
    ")\n",
    "transformer.transform(data=test_x_s3, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics via (Batch Transform)\n",
      "Accuracy : 0.7595\n",
      "Precision: 0.6086\n",
      "Recall   : 0.1311\n",
      "F1-score : 0.2157\n",
      "ROC AUC  : 0.6892\n",
      "Confusion matrix [[TN FP]\n",
      " [FN TP]]:\n",
      "[[22882   670]\n",
      " [ 6906  1042]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "s3r = boto3.resource(\"s3\"); bkt = s3r.Bucket(bucket)\n",
    "outs = [o.key for o in bkt.objects.filter(Prefix=f\"{prefix}batch_out/\") if o.key.endswith(\".out\")]\n",
    "\n",
    "y_pred, y_prob = [], []\n",
    "for key in outs:\n",
    "    for line in bkt.Object(key).get()[\"Body\"].read().decode(\"utf-8\").strip().splitlines():\n",
    "        rec = json.loads(line)\n",
    "        prob = rec.get(\"score\", rec.get(\"scores\", [None])[0])\n",
    "        lab  = rec.get(\"predicted_label\", int(prob >= 0.5) if prob is not None else 0)\n",
    "        y_prob.append(prob if prob is not None else float(lab))\n",
    "        y_pred.append(int(lab))\n",
    "\n",
    "y_prob = np.array(y_prob, float); y_pred = np.array(y_pred, int)\n",
    "assert len(y_true)==len(y_pred)==len(y_prob), \"length mismatch\"\n",
    "\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "try: auc = roc_auc_score(y_true, y_prob)\n",
    "except ValueError: auc = float(\"nan\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Test metrics via (Batch Transform)\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "print(f\"ROC AUC  : {auc:.4f}\")\n",
    "print(\"Confusion matrix [[TN FP]\\n [FN TP]]:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Build and evaluate ensembe models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use xgboost estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "6. write down your observation on the difference between the performance of using the simple and ensemble models.\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve XGBoost image from previous\n",
    "container = image_uris.retrieve(framework=\"xgboost\", region=region, version=\"1.5-1\")\n",
    "\n",
    "# Define the estimator\n",
    "xgb_est = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}xgb_output/\",\n",
    "    sagemaker_session=sess,\n",
    "    max_run=1800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple hyperparameters for binary classification\n",
    "xgb_est.set_hyperparameters(\n",
    "    objective=\"binary:logistic\", \n",
    "    num_round=50,               \n",
    "    max_depth=4,                 \n",
    "    eta=0.2,                   \n",
    "    subsample=0.8,\n",
    "    eval_metric=\"logloss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-10-27-10-19-12-482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-27 10:19:13 Starting - Starting the training job...\n",
      "2025-10-27 10:19:27 Starting - Preparing the instances for training...\n",
      "2025-10-27 10:19:51 Downloading - Downloading input data...\n",
      "2025-10-27 10:20:41 Downloading - Downloading the training image......\n",
      "2025-10-27 10:21:32 Training - Training image download completed. Training in progress....\n",
      "2025-10-27 10:22:03 Uploading - Uploading generated training model...\n",
      "2025-10-27 10:22:21 Completed - Training job completed\n",
      "..Training seconds: 150\n",
      "Billable seconds: 150\n"
     ]
    }
   ],
   "source": [
    "# Point to training and validation sets in S3\n",
    "train_input = TrainingInput(f\"s3://{bucket}/{prefix}train.csv\", content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(f\"s3://{bucket}/{prefix}val.csv\",   content_type=\"text/csv\")\n",
    "\n",
    "# Start training job\n",
    "xgb_est.fit({\"train\": train_input, \"validation\": val_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-27-10-23-05-122\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2025-10-27-10-23-05-122\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2025-10-27-10-23-05-122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!Deployed XGBoost endpoint: sagemaker-xgboost-2025-10-27-10-23-05-122\n"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb_est.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "endpoint_name = xgb_predictor.endpoint_name\n",
    "print(\"Deployed XGBoost endpoint:\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Transform on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-27-10-27-07-499\n",
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2025-10-27-10-27-08-074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "..."
     ]
    }
   ],
   "source": [
    "transformer = xgb_est.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}xgb_batch_out/\",\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"application/jsonlines\",\n",
    ")\n",
    "\n",
    "test_x_s3 = f\"s3://{bucket}/{prefix}test_x.csv\"\n",
    "transformer.transform(data=test_x_s3, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics via (XGBoost Batch Transform)\n",
      "Accuracy : 0.7764\n",
      "Precision: 0.6887\n",
      "Recall   : 0.2080\n",
      "F1-score : 0.3195\n",
      "ROC AUC  : 0.7407\n",
      "Confusion matrix [[TN FP]\n",
      " [FN TP]]:\n",
      "[[22805   747]\n",
      " [ 6295  1653]]\n"
     ]
    }
   ],
   "source": [
    "import re, json, io, boto3, pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "s3r = boto3.resource(\"s3\")\n",
    "bkt = s3r.Bucket(bucket)\n",
    "\n",
    "# collect all .out shards (sort for deterministic order)\n",
    "outs = sorted([o.key for o in bkt.objects.filter(Prefix=f\"{prefix}xgb_batch_out/\") if o.key.endswith(\".out\")])\n",
    "\n",
    "def parse_prob(line: str):\n",
    "    \"\"\"Return a float probability from a batch transform output line.\n",
    "    Handles: plain number, CSV, JSON dict/list.\"\"\"\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "    # 1) try JSON\n",
    "    try:\n",
    "        obj = json.loads(line)\n",
    "        if isinstance(obj, (int, float)):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, dict):\n",
    "            # common keys\n",
    "            for k in (\"score\", \"probability\", \"prediction\", \"predicted_value\"):\n",
    "                if k in obj and isinstance(obj[k], (int, float)):\n",
    "                    return float(obj[k])\n",
    "            # some models put a list under 'scores' or 'predictions'\n",
    "            for k in (\"scores\", \"predictions\"):\n",
    "                v = obj.get(k)\n",
    "                if isinstance(v, list) and len(v) and isinstance(v[0], (int, float)):\n",
    "                    return float(v[0])\n",
    "            # fallback: first numeric value found\n",
    "            for v in obj.values():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    return float(v)\n",
    "        if isinstance(obj, list):\n",
    "            # first numeric entry\n",
    "            for v in obj:\n",
    "                if isinstance(v, (int, float)):\n",
    "                    return float(v)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # 2) try CSV / whitespace-separated\n",
    "    parts = [p for p in re.split(r\"[,\\s]+\", line) if p]\n",
    "    for p in parts:\n",
    "        try:\n",
    "            return float(p)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "y_pred, y_prob = [], []\n",
    "for key in outs:\n",
    "    body = bkt.Object(key).get()[\"Body\"].read().decode(\"utf-8\")\n",
    "    for line in body.splitlines():\n",
    "        prob = parse_prob(line)\n",
    "        if prob is None:  # skip empty/garbage lines safely\n",
    "            continue\n",
    "        y_prob.append(prob)\n",
    "        y_pred.append(int(prob >= 0.5))\n",
    "\n",
    "# load ground-truth labels (first column is label)\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=bucket, Key=f\"{prefix}test.csv\")\n",
    "test_df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()), header=None)\n",
    "y_true = test_df.iloc[:, 0].astype(int).values\n",
    "\n",
    "# sanity check: length match\n",
    "if len(y_true) != len(y_pred):\n",
    "    print(f\"warning: length mismatch — labels={len(y_true)}, preds={len(y_pred)}. \"\n",
    "          f\"Did batch split your test into multiple shards or include a header?\")\n",
    "    # you can bail out or truncate to min length:\n",
    "    n = min(len(y_true), len(y_pred))\n",
    "    y_true, y_pred, y_prob = y_true[:n], y_pred[:n], y_prob[:n]\n",
    "\n",
    "# metrics\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "except ValueError:\n",
    "    auc = float(\"nan\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Test metrics via (XGBoost Batch Transform)\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "print(f\"ROC AUC  : {auc:.4f}\")\n",
    "print(\"Confusion matrix [[TN FP]\\n [FN TP]]:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3r = boto3.resource(\"s3\")\n",
    "bkt = s3r.Bucket(bucket)\n",
    "\n",
    "outs = [o.key for o in bkt.objects.filter(Prefix=f\"{prefix}xgb_batch_out/\") if o.key.endswith(\".out\")]\n",
    "\n",
    "y_pred, y_prob = [], []\n",
    "\n",
    "for key in outs:\n",
    "    body = bkt.Object(key).get()[\"Body\"].read().decode(\"utf-8\").strip().splitlines()\n",
    "    for line in body:\n",
    "        try:\n",
    "            rec = json.loads(line)\n",
    "            if isinstance(rec, dict):\n",
    "                prob = rec.get(\"score\", rec.get(\"scores\", [None])[0])\n",
    "            else:\n",
    "                prob = float(rec)\n",
    "        except Exception:\n",
    "            try:\n",
    "                prob = float(line.strip())\n",
    "            except:\n",
    "                continue\n",
    "        if prob is None:\n",
    "            continue\n",
    "        y_prob.append(prob)\n",
    "        y_pred.append(int(prob >= 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics via (XGBoost Batch Transform)\n",
      "Accuracy : 0.7764\n",
      "Precision: 0.6887\n",
      "Recall   : 0.2080\n",
      "F1-score : 0.3195\n",
      "ROC AUC  : 0.7407\n",
      "Confusion matrix [[TN FP]\n",
      " [FN TP]]:\n",
      "[[22805   747]\n",
      " [ 6295  1653]]\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=bucket, Key=f\"{prefix}test.csv\")\n",
    "test_df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()), header=None)\n",
    "y_true = test_df.iloc[:, 0].astype(int).values\n",
    "\n",
    "# Metrics\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "print(\"Test metrics via (XGBoost Batch Transform)\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "print(f\"ROC AUC  : {auc:.4f}\")\n",
    "print(\"Confusion matrix [[TN FP]\\n [FN TP]]:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
